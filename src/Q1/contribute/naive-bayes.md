## ä½¿ç”¨Rustå®ç°æœ´ç´ è´å¶æ–¯ï¼ˆNaive Bayesï¼‰ç®—æ³•

ä½œè€…ï¼š ç‹æ±Ÿæ¡ã€å‘¨ç´«é¹

---

åœ¨è®¨è®ºå…·ä½“ä»£ç ä¹‹å‰ï¼Œé¦–å…ˆè®©æˆ‘ä»¬å…ˆäº†è§£ä¸€ä¸‹ç›¸å…³æ¦‚å¿µã€‚

### æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ

æ¦‚ç‡å¯ä»¥åæ˜ éšæœºäº‹ä»¶å‡ºç°çš„å¯èƒ½æ€§å¤§å°ï¼Œè€Œéšæœºäº‹ä»¶æ˜¯æŒ‡åœ¨ç›¸åŒæ¡ä»¶ä¸‹ï¼Œå¯èƒ½å‡ºç°ä¹Ÿå¯èƒ½ä¸å‡ºç°çš„äº‹ä»¶ã€‚æ¡ä»¶æ¦‚ç‡åˆ™æŒ‡ï¼Œåœ¨æŸä¸€éšæœºäº‹ä»¶å·²ç»å‘ç”Ÿçš„å‰æä¸‹ï¼Œå¦ä¸€éšæœºäº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡ï¼Œå†™ä½œ`P(A | B)`ã€‚å¦‚æœç”¨éŸ¦æ©å›¾è¡¨ç¤ºï¼Œå¯ä»¥å°†äº‹ä»¶`A`ã€`B`å‘ç”Ÿçš„æ¦‚ç‡è¡¨ç¤ºå¦‚ä¸‹ï¼š

![image-20220205221629260](./images/bayes/image-20220205221629260.png)

å½“`P(B) â‰  0`æ—¶ï¼Œé‚£ä¹ˆï¼š
$$
P(A\ |\ B) = \frac{P(A\ âˆ©\ B)}{P(B)}
$$
å½“äº‹ä»¶`B`çš„å‘ç”Ÿæ¦‚ç‡ä¸º0æ—¶ï¼Œæ¡ä»¶æ¦‚ç‡`P(A | B)`å€¼ä¸ºæœªå®šä¹‰ã€‚

å½“äº‹ä»¶`A`ä¸`B`å®Œå…¨ç‹¬ç«‹æ—¶ï¼Œ`P(A | B) = P(A)`ã€‚



### è´å¶æ–¯å®šç†ï¼ˆBayes Theoremï¼‰

æœ‰äº†ä¸Šè¿°ç­‰å¼ä»¥åï¼Œå½“æˆ‘ä»¬å¾—çŸ¥äº‹ä»¶`B`å‘ç”Ÿæƒ…å†µä¸‹ï¼Œæ‰€æœ‰äº‹ä»¶`Ai`çš„å‘ç”Ÿæ¦‚ç‡æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—äº‹ä»¶`B`çš„å‘ç”Ÿæ¦‚ç‡ã€‚ä¾‹å¦‚ï¼š

![image-20220205222327064](./images/bayes/image-20220205222327064.png)

äº‹ä»¶`B`çš„æ¦‚ç‡è®¡ç®—å¦‚ä¸‹ï¼š
$$
\begin{aligned}
	P(B) =\ & \ \ \ \ \  P(A_1) \cdot P(B\ |\ A_1) \\
		    &+ P(A_2) \cdot P(B\ |\ A_2) \\
		    &+ P(A_3) \cdot P(B\ |\ A_3) \\
\end{aligned}
$$
è´å¶æ–¯å®šç†åˆ™åœ¨æ­¤äºŒè€…åŸºç¡€ä¸Šæ¨è®ºï¼Œå½“æˆ‘ä»¬çŸ¥é“äº‹ä»¶`Ai`çš„å‘ç”Ÿæ¦‚ç‡ï¼Œä»¥åŠäº‹ä»¶`B`çš„æ¡ä»¶æ¦‚ç‡ï¼Œå³å½“`P(Ai)`ä¸”`P(B | Ai)`å·²çŸ¥æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—äº‹ä»¶`Ai`çš„æ¡ä»¶æ¦‚ç‡ï¼Œå³`P(Ai | B)`ï¼š
$$
\begin{aligned}
	P(A_i \ | \ B) &= \frac{P(A_i âˆ© B)}{P(B)} \\ 
				   &= \frac{P(A_i) \cdot P(B \ | \ A_i)}{P(B)} \\ 
				   &= \frac{P(A_i) \cdot P(B \ | \ A_i)}{\sum_j^i P(A_j) \cdot P(B \ | \ A_j)} \\ 
\end{aligned}
$$



### æœ´ç´ è´å¶æ–¯ï¼ˆNaive Bayesï¼‰ä¸æœºå™¨å­¦ä¹ 

å¯¹äºè´å¶æ–¯å®šç†ï¼Œæ¢ä¸ªè§’åº¦è€Œè¨€ï¼Œäº‹ä»¶`A`ä¹Ÿå¯ä»¥çœ‹ä½œæ˜¯è°ƒæŸ¥æ ·æœ¬`D`çš„é›†åˆï¼Œ`Ai`åˆ™æ˜¯å…¶ä¸­å•ç‹¬çš„ä¸€ä»½æ ·æœ¬ï¼Œäº‹ä»¶`B`åˆ™å¯ä»¥è®¤ä¸ºæ˜¯æŸä¸ªç»“è®ºï¼Œæˆ–æŸç§åˆ†ç±»ç»“æœï¼Œå³å‡è¯´`H`ã€‚å› æ­¤ï¼Œé€šè¿‡è®¡ç®—å‡è¯´`H`æˆç«‹æ—¶ï¼Œæ ·æœ¬å‡ºç°çš„æ¦‚ç‡ï¼Œå³è®¡ç®—`P(Di | Hj)`ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å®šç†è®¡ç®—`P(Hj | Di)`ï¼Œå³åœ¨å‡ºç°æ ·æœ¬`Di`çš„æƒ…å†µä¸‹ï¼Œå‡è¯´`H`çš„`j`åˆ†æ”¯æˆç«‹æ¦‚ç‡æ˜¯å¤šå°‘ã€‚å› æ­¤ï¼Œå½“æ–°æ ·æœ¬`Dx`å‡ºç°æ—¶ï¼Œè®¡ç®—æœºå¯ä»¥é€šè¿‡è´å¶æ–¯å®šç†è®¡ç®—ï¼Œåœ¨å‡ºç°è¯¥æ ·æœ¬`Dx`çš„å‰æä¸‹ï¼Œå‡è¯´`H`æ‰€æœ‰åˆ†æ”¯`j`æˆç«‹çš„æ¦‚ç‡ã€‚è®¡ç®—å‡ºè¿™ä¸ªæ¦‚ç‡ä¹‹åï¼Œè®¡ç®—æœºå¯ä»¥é€‰æ‹©æœ€é«˜æ¦‚ç‡çš„åˆ†æ”¯ï¼Œå¹¶ä¸”é¢„æµ‹æ–°æ ·æœ¬`Dx`å±äºè¯¥åˆ†æ”¯ã€‚

å¤§è‡´æ¥è¯´ï¼Œè¿™å°±æ˜¯æœºå™¨å­¦ä¹ ä¸­çš„æœ´ç´ è´å¶æ–¯æ¨¡å‹ã€‚åœ¨ç»™å®šæ ·æœ¬é›†`D`çš„æƒ…å†µä¸‹ï¼Œåˆ™ï¼š
$$
\begin{aligned}
	P(H = j_1 \ | \ D_x) &= \frac{P(H = j_1 \ âˆ© \ D_x)}{P(D_x)} = \frac{P(D_x \ | \ H = j_1) \cdot P(H = j_1)}{P(D_x)} \\
	P(H = j_2 \ | \ D_x) &= \frac{P(H = j_2 \ âˆ© \ D_x)}{P(D_x)} = \frac{P(D_x \ | \ H = j_2) \cdot P(H = j_2)}{P(D_x)} \\
	... \\
	P(H = j_m \ | \ D_x) &= \frac{P(H = j_m \ âˆ© \ D_x)}{P(D_x)} = \frac{P(D_x \ | \ H = j_m) \cdot P(H = j_m)}{P(D_x)} \\
\end{aligned}
$$

å› æ­¤ï¼Œåˆ¤å®šæ–°æ ·æœ¬`Dx`å±äºå“ªä¸ªåˆ†æ”¯ï¼Œç­‰åŒäºè®¡ç®—`P(Dx | H = j1) Â· P(H = j1)`ã€`P(Dx | H = j2) Â· P(H = j2)`ã€...ã€`P(Dx | H = jm) Â· P(H = jm)`ä¸­çš„æœ€å¤§å€¼ã€‚`P(H = jm)`ä¸`P(Dx | H = jm)`çš„æ¦‚ç‡å¯ä»¥ç”¨æ ·æœ¬ä¸­è¯¥åˆ†æ”¯å‡ºç°çš„é¢‘ç‡ä»¥åŠæ ·æœ¬çš„æ¡ä»¶æ¦‚ç‡æ¨¡æ‹Ÿã€‚ä¸è¿‡ï¼Œæœ´ç´ è´å¶æ–¯æ¨¡å‹ä¾èµ–äºä¸€ä¸ªå¤§å‰æï¼šæ¯ä¸ªæ ·æœ¬çš„å‡ºç°éƒ½æ˜¯å®Œå…¨ç‹¬ç«‹ä¸”éšæœºçš„ã€‚å¦‚æœå®ƒä»¬å¹¶ä¸æ˜¯ç‹¬ç«‹ä¸”éšæœºçš„ï¼Œæ¦‚ç‡æ¨¡æ‹Ÿçš„å‡†ç¡®ç¨‹åº¦å°±ä¼šå—åˆ°å½±å“ï¼Œå¯¼è‡´åˆ†ç±»å™¨ç»“æœæ›´å®¹æ˜“å‡ºç°é”™è¯¯ã€‚

æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨ï¼ˆNaive Bayes Classifierï¼‰çš„åº”ç”¨ä¹‹ä¸€æ˜¯åƒåœ¾é‚®ä»¶åˆ†ç±»å™¨ã€‚ä¸€ä¸ªæ¯”è¾ƒç®€å•çš„åƒåœ¾é‚®ä»¶åˆ†ç±»å™¨çš„å·¥ä½œåŸç†å¦‚ä¸‹ï¼š

ç»™å®šé‚®ä»¶æ•°æ®é›†`D`ï¼Œä»¥åŠå®ƒä»¬å¯¹åº”çš„äºŒå…ƒæ ‡ç­¾å€¼`T`ã€‚å·²çŸ¥ï¼Œå½“`T = 0`æ—¶ï¼Œè¯¥é‚®ä»¶ä¸æ˜¯åƒåœ¾é‚®ä»¶ï¼Œå½“`T = 1`æ—¶ï¼Œè¯¥é‚®ä»¶æ˜¯åƒåœ¾é‚®ä»¶ã€‚å°†é‚®ä»¶ä¸­æ‰€æœ‰å‡ºç°çš„è¯ï¼Œè®¤ä¸ºæ˜¯è¯¥é‚®ä»¶çš„ç‰¹å¾`F`ã€‚æœ‰äº†ä»¥ä¸Šä¿¡æ¯ä»¥åï¼Œæˆ‘ä»¬å¯ä»¥æ¨¡æ‹Ÿè®¡ç®—ï¼š

- `P(T)`ï¼Œå³åœ¨è¿™äº›é‚®ä»¶ä¸­ï¼Œ`T = 1`ä»¥åŠ`T = 0`çš„æ¦‚ç‡ï¼›
- `P(F | T)`ï¼Œå³åœ¨è¿™äº›é‚®ä»¶ä¸­ï¼Œå½“é‚®ä»¶æ˜¯åƒåœ¾é‚®ä»¶æˆ–éåƒåœ¾é‚®ä»¶æ—¶ï¼ŒæŸä¸ªç‰¹å¾å‡ºç°çš„æ¦‚ç‡ï¼›
- å½“è®¤ä¸ºç‰¹å¾ä»¥åŠæ•°æ®ä¹‹é—´éƒ½æ˜¯ç›¸äº’ç‹¬ç«‹ä¸”éšæœºçš„æ—¶å€™ï¼Œ`P(Fi âˆ© Fj | T) = P(Fi | T) Â· P(Fj | T)`ã€‚å°½ç®¡åœ¨å®é™…è¯­è¨€ä¸­ï¼Œä¸¤ä¸ªè¯è¯­ä¹‹é—´å¹¶éå®Œå…¨ç‹¬ç«‹ä¸”éšæœºå‡ºç°ï¼Œä½†æ˜¯ä¸ºäº†ä¾¿åˆ©ï¼Œæš‚ä¸”ä»¥è¿™ç§æ–¹å¼è®¡ç®—ã€‚å› æ­¤ï¼Œå¯ä»¥è®¡ç®—`P(Fj0 âˆ© Fj1 âˆ© Fj2 âˆ© ... âˆ© Fjn, Fji âˆˆ Dj | T) `ï¼Œå³ï¼Œåˆ¤å®šé‚®ä»¶æ˜¯åƒåœ¾é‚®ä»¶çš„æƒ…å†µä¸‹ï¼Œè¯¥é‚®ä»¶`Dj`å‡ºç°çš„æ¦‚ç‡ï¼Œä»¥åŠåˆ¤å®šéåƒåœ¾é‚®ä»¶çš„æƒ…å†µä¸‹ï¼Œè¯¥é‚®ä»¶å‡ºç°çš„æ¦‚ç‡ã€‚

å†ä¸€æ¬¡ï¼Œè®¡ç®—å‡ºä»¥ä¸Šä¿¡æ¯ä»¥åï¼Œå¯¹äºæ–°é‚®ä»¶`Dx`ï¼Œé€šè¿‡è´å¶æ–¯å®šç†ï¼Œæˆ‘ä»¬å¯ä»¥æ¨¡æ‹Ÿè®¡ç®—`P(T | Dx)`ï¼Œå³ç»™å®šè¯¥é‚®ä»¶ï¼Œæ˜¯æˆ–ä¸æ˜¯åƒåœ¾é‚®ä»¶çš„æ¦‚ç‡ï¼Œä»è€Œå¯¹å…¶è¿›è¡Œåˆ†ç±»ã€‚ä¸è¿‡ï¼Œå®é™…è®¡ç®—ä¸­ï¼Œåªéœ€è¦è®¡ç®—`P(Dx | T) Â· P(T)`å¹¶è¿›è¡Œæ¯”è¾ƒï¼Œå¹¶ä¸éœ€è¦è®¡ç®—`P(Dx)`ã€‚

è™½ç„¶å¦‚æ­¤ï¼Œè®¡ç®—ä¸­ä»ç„¶å­˜åœ¨ä¸€ä¸ªé—®é¢˜ï¼šå¦‚æœé‚®ä»¶`Dx`ä¸­å­˜åœ¨æ ·æœ¬é‚®ä»¶ä¸­æ²¡æœ‰å‡ºç°è¿‡çš„è¯æ±‡ï¼Œæˆ–æ˜¯åƒåœ¾é‚®ä»¶/éåƒåœ¾é‚®ä»¶ä¸­æœªå‡ºç°çš„è¯æ±‡ï¼Œé‚£ä¹ˆ`P(F | T)`çš„å€¼ä¸º0ï¼Œè¿™ä¼šå¯¼è‡´æ•´ä¸ªè®¡ç®—å¼çš„ä¹˜æ³•ç»“æœä¸º0ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å¹¶ä¸èƒ½ç›´æ¥åˆ¤å®šè¿™ä¸ªè¯æ±‡å¯ä»¥ç›´æ¥å†³å®šè¯¥é‚®ä»¶çš„æ ‡è®°ï¼Œå› ä¸ºæˆ‘ä»¬å¹¶æ²¡æœ‰ç©·å°½æ‰€æœ‰çš„æ ·æœ¬ã€‚ä¸ºäº†è°ƒæ•´è¿™ä¸ªåå·®ï¼Œå¯ä»¥ä½¿ç”¨æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘å¤„ç†ï¼ˆLaplace Smoothingï¼‰æ¥è°ƒæ•´æ¦‚ç‡ä¸ä¸º0ã€‚æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘å¤„ç†ä¸ºï¼Œäººä¸ºè§„å®šæ•°å€¼`Î±`ï¼Œåœ¨è®¡ç®—æ¦‚ç‡æ—¶ï¼Œåœ¨åˆ†å­ä½ç½®åŠ ä¸Š`Î±`ï¼Œåœ¨åˆ†æ¯ä½åˆ™åŠ ä¸Š`2Î±`ã€‚åœ¨å¾ˆå¤šåœºæ™¯ä¸­ï¼Œ`Î± = 1`ã€‚



### ä½¿ç”¨Rustå®ç°æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨

å¯¹äºç»™å®šé‚®ä»¶å­—ç¬¦ä¸²é›†åˆä»¥åŠå®ƒä»¬å¯¹åº”çš„æ ‡è®°ï¼Œé¦–å…ˆæˆ‘ä»¬éœ€è¦å°†å…¶è½¬åŒ–æˆå•ç‹¬çš„è¯æ±‡ï¼ˆTokenï¼‰ï¼Œå³è·å–é‚®ä»¶æ•°æ®çš„ç‰¹å¾ï¼Œä»è€Œè®¡ç®—ç›¸åº”æ¦‚ç‡ã€‚ç›¸å¯¹æ›´å¤æ‚ä¸€ç‚¹çš„è½¬æ¢å¯ä»¥æ¶‰åŠè¯é¢‘ç”šè‡³æ˜¯è¯æ±‡çš„å¤§å°å†™ï¼Œä¸è¿‡å¦‚æœåªæ˜¯å†™ä¸€ä¸ªæœ€åŸºç¡€çš„æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨ï¼Œä¹Ÿå¯ä»¥æš‚æ—¶å…ˆä¸è€ƒè™‘è¿™äº›å› ç´ ã€‚æœ¬æ–‡ä¸­ä»‹ç»çš„å®ç°æ–¹æ³•ä¸»è¦æ¥è‡ªäºã€Š[How to Implement a Naive Bayes Classifier with Rust](https://www.freecodecamp.org/news/implement-naive-bayes-with-rust/)ã€‹ä¸€æ–‡ï¼Œæ–‡ä¸­ï¼Œä½œè€…ä½¿ç”¨äº†Regexåº“ï¼Œé€šè¿‡åŒ¹é…æ­£åˆ™è¡¨è¾¾å¼æ¥è§£æè¯ç»„ã€‚Regexåº“çš„å…·ä½“ä»‹ç»å¯è§Rust Cook Book[ç¬¬18ç« ](https://rust-lang-nursery.github.io/rust-cookbook/text/regex.html)ï¼Œåœ¨æ­¤æš‚æ—¶ä¸å±•å¼€äº†ã€‚

è§£æä¹‹åï¼Œæœ€åŸå§‹çš„åšæ³•æ˜¯ï¼Œä½¿ç”¨`std::collections::HashSet`å­˜å‚¨æ‰€æœ‰çš„è¯æ±‡ï¼Œä½¿ç”¨`std::collections::HashMap`å­˜å‚¨è¯æ±‡ä»¥åŠå¯¹åº”çš„æ¦‚ç‡ã€‚æ–°å»ºä¸¤ä¸ªç»“æ„ä½“å¦‚ä¸‹ï¼š

```rust
// å•ä¸€é‚®ä»¶æ ·æœ¬
pub struct Message<'a> {
    pub text: &'a str,
    pub is_spam: bool,
}

// åˆ†ç±»å™¨
pub struct NaiveBayesClassifier {
    pub alpha: f64,
    pub tokens: HashSet<String>,
    pub token_ham_counts: HashMap<String, i32>,
    pub token_spam_counts: HashMap<String, i32>,
    pub spam_messages_count: i32,
    pub ham_messages_count: i32,
}
```

è®­ç»ƒæœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨åˆ™ç›¸å½“äºä¸€ä¸€è§£æé‚®ä»¶æ ·æœ¬ï¼Œå¹¶ä¸”æ›´æ–°å¯¹åº”çš„æ•°æ®ï¼Œä»¥ä¾¿åœ¨é¢„æµ‹æ–°æ ·æœ¬æ—¶è¿›è¡Œè®¡ç®—ã€‚ä»£ç æ¥è‡ªäºåŒä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæ‘˜å½•å¦‚ä¸‹ï¼š

```rust
impl NaiveBayesClassifier {
    pub fn train(&mut self, messages: &[Message]) {
        for message in messages.iter() {
            self.increment_message_classifications_count(message);
            for token in tokenize(&message.text.to_lowercase()) {
                self.tokens.insert(token.to_string());
                self.increment_token_count(token, message.is_spam)
            }
        }
    }

    fn increment_message_classifications_count(&mut self, message: &Message) {
        if message.is_spam {
            self.spam_messages_count += 1;
        } else {
            self.ham_messages_count += 1;
        }
    }

    fn increment_token_count(&mut self, token: &str, is_spam: bool) {
        if !self.token_spam_counts.contains_key(token) {
            self.token_spam_counts.insert(token.to_string(), 0);
        }

        if !self.token_ham_counts.contains_key(token) {
            self.token_ham_counts.insert(token.to_string(), 0);
        }

        if is_spam {
            self.increment_spam_count(token);
        } else {
            self.increment_ham_count(token);
        }
    }

    fn increment_spam_count(&mut self, token: &str) {
        *self.token_spam_counts.get_mut(token).unwrap() += 1;
    }

    fn increment_ham_count(&mut self, token: &str) {
        *self.token_ham_counts.get_mut(token).unwrap() += 1;
    }
}
```

é¢„æµ‹æ—¶ï¼Œå¥—ç”¨ä¸Šè¿°å…¬å¼ï¼Œå¯ä»¥è®¡ç®—å‡º`P(Dx | T) Â· P(T)`ã€‚åœ¨è¿›è¡Œ`P(Fi âˆ© Fj | T) = P(Fi | T) Â· P(Fj | T)`è®¡ç®—æ—¶ï¼Œç”±äºæ¦‚ç‡å¤§å¤šéƒ½æ˜¯å°æ•°ï¼Œè¿ç»­å°æ•°çš„ä¹˜ç§¯å¯èƒ½ä¼šå¯¼è‡´ç²¾åº¦ä¸¢å¤±ï¼ˆUnderflowï¼‰ã€‚å› æ­¤ï¼Œå°†ä¹˜ç§¯è®¡ç®—æ›¿æ¢ä¸ºå¯¹æ•°è®¡ç®—ï¼Œå³ï¼š
$$
\begin{aligned}
	P(F_0 \ âˆ© \ F_1 \ âˆ© \ ... \ âˆ© \ F_n \ | \ T) &= P(F_0 \ | \ T) \cdot P(F_1 \ | \ T) \cdot \ ... \ \cdot P(F_n \ | \ T) \\ 
	&â†’ ln( \ P(F_0 \ | \ T)\cdot P(F_1 \ | \ T) \cdot \ ... \ \cdot P(F_n \ | \ T) \ ) \\
	&= ln(P(F_0 \ | \ T)) \  + \ ln(P(F_1 \ | \ T)) \ + \ ... \ + \ ln(P(F_n \ | \ T))
\end{aligned}
$$


å¯¹åº”çš„ä»£ç æ‘˜å½•å¦‚ä¸‹ï¼š

```rust
impl NaiveBayesClassifier {
	pub fn predict(&self, text: &str) -> f64 {
        let lower_case_text = text.to_lowercase();
        let message_tokens = tokenize(&lower_case_text);
        let (prob_if_spam, prob_if_ham) = self.probabilities_of_message(message_tokens);

        return prob_if_spam / (prob_if_spam + prob_if_ham);
    }

    fn probabilities_of_message(&self, message_tokens: HashSet<&str>) -> (f64, f64) {
        let mut log_prob_if_spam = 0.;
        let mut log_prob_if_ham = 0.;

        for token in self.tokens.iter() {
            let (prob_if_spam, prob_if_ham) = self.probabilites_of_token(&token);

            if message_tokens.contains(token.as_str()) {
                log_prob_if_spam += prob_if_spam.ln();
                log_prob_if_ham += prob_if_ham.ln();
            } else {
                log_prob_if_spam += (1. - prob_if_spam).ln();
                log_prob_if_ham += (1. - prob_if_ham).ln();
            }
        }

        let prob_if_spam = log_prob_if_spam.exp();
        let prob_if_ham = log_prob_if_ham.exp();

        return (prob_if_spam, prob_if_ham);
    }

    fn probabilites_of_token(&self, token: &str) -> (f64, f64) {
        let prob_of_token_spam = (self.token_spam_counts[token] as f64 + self.alpha)
            / (self.spam_messages_count as f64 + 2. * self.alpha);

        let prob_of_token_ham = (self.token_ham_counts[token] as f64 + self.alpha)
            / (self.ham_messages_count as f64 + 2. * self.alpha);

        return (prob_of_token_spam, prob_of_token_ham);
    }
}
```

æœ€åï¼Œä¸€ä¸ªå¯èƒ½çš„æµ‹è¯•æ ·æœ¬ï¼š

```rust
// ...lib.rs

pub fn new_classifier(alpha: f64) -> NaiveBayesClassifier {
    return NaiveBayesClassifier {
        alpha,
        tokens: HashSet::new(),
        token_ham_counts: HashMap::new(),
        token_spam_counts: HashMap::new(),
        spam_messages_count: 0,
        ham_messages_count: 0,
    };
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn naive_bayes() {
        let train_messages = [
            Message {
                text: "Free Bitcoin viagra XXX christmas deals ğŸ˜»ğŸ˜»ğŸ˜»",
                is_spam: true,
            },
            Message {
                text: "My dear Granddaughter, please explain Bitcoin over Christmas dinner",
                is_spam: false,
            },
            Message {
                text: "Here in my garage...",
                is_spam: true,
            },
        ];

        let alpha = 1.;
        let num_spam_messages = 2.;
        let num_ham_messages = 1.;

        let mut model = new_classifier(alpha);
        model.train(&train_messages);

        let mut expected_tokens: HashSet<String> = HashSet::new();
        for message in train_messages.iter() {
            for token in tokenize(&message.text.to_lowercase()) {
                expected_tokens.insert(token.to_string());
            }
        }

        let input_text = "Bitcoin crypto academy Christmas deals";

        let probs_if_spam = [
            1. - (1. + alpha) / (num_spam_messages + 2. * alpha), // "Free"  (not present)
            (1. + alpha) / (num_spam_messages + 2. * alpha),      // "Bitcoin"  (present)
            1. - (1. + alpha) / (num_spam_messages + 2. * alpha), // "viagra"  (not present)
            1. - (1. + alpha) / (num_spam_messages + 2. * alpha), // "XXX"  (not present)
            (1. + alpha) / (num_spam_messages + 2. * alpha),      // "christmas"  (present)
            (1. + alpha) / (num_spam_messages + 2. * alpha),      // "deals"  (present)
            1. - (1. + alpha) / (num_spam_messages + 2. * alpha), // "my"  (not present)
            1. - (0. + alpha) / (num_spam_messages + 2. * alpha), // "dear"  (not present)
            1. - (0. + alpha) / (num_spam_messages + 2. * alpha), // "granddaughter"  (not present)
            1. - (0. + alpha) / (num_spam_messages + 2. * alpha), // "please"  (not present)
            1. - (0. + alpha) / (num_spam_messages + 2. * alpha), // "explain"  (not present)
            1. - (0. + alpha) / (num_spam_messages + 2. * alpha), // "over"  (not present)
            1. - (0. + alpha) / (num_spam_messages + 2. * alpha), // "dinner"  (not present)
            1. - (1. + alpha) / (num_spam_messages + 2. * alpha), // "here"  (not present)
            1. - (1. + alpha) / (num_spam_messages + 2. * alpha), // "in"  (not present)
            1. - (1. + alpha) / (num_spam_messages + 2. * alpha), // "garage"  (not present)
        ];

        let probs_if_ham = [
            1. - (0. + alpha) / (num_ham_messages + 2. * alpha), // "Free"  (not present)
            (1. + alpha) / (num_ham_messages + 2. * alpha),      // "Bitcoin"  (present)
            1. - (0. + alpha) / (num_ham_messages + 2. * alpha), // "viagra"  (not present)
            1. - (0. + alpha) / (num_ham_messages + 2. * alpha), // "XXX"  (not present)
            (1. + alpha) / (num_ham_messages + 2. * alpha),      // "christmas"  (present)
            (0. + alpha) / (num_ham_messages + 2. * alpha),      // "deals"  (present)
            1. - (1. + alpha) / (num_ham_messages + 2. * alpha), // "my"  (not present)
            1. - (1. + alpha) / (num_ham_messages + 2. * alpha), // "dear"  (not present)
            1. - (1. + alpha) / (num_ham_messages + 2. * alpha), // "granddaughter"  (not present)
            1. - (1. + alpha) / (num_ham_messages + 2. * alpha), // "please"  (not present)
            1. - (1. + alpha) / (num_ham_messages + 2. * alpha), // "explain"  (not present)
            1. - (1. + alpha) / (num_ham_messages + 2. * alpha), // "over"  (not present)
            1. - (1. + alpha) / (num_ham_messages + 2. * alpha), // "dinner"  (not present)
            1. - (0. + alpha) / (num_ham_messages + 2. * alpha), // "here"  (not present)
            1. - (0. + alpha) / (num_ham_messages + 2. * alpha), // "in"  (not present)
            1. - (0. + alpha) / (num_ham_messages + 2. * alpha), // "garage"  (not present)
        ];

        let p_if_spam_log: f64 = probs_if_spam.iter().map(|p| p.ln()).sum();
        let p_if_spam = p_if_spam_log.exp();

        let p_if_ham_log: f64 = probs_if_ham.iter().map(|p| p.ln()).sum();
        let p_if_ham = p_if_ham_log.exp();

        // P(message | spam) / (P(messge | spam) + P(message | ham)) rounds to 0.97
        assert!((model.predict(input_text) - p_if_spam / (p_if_spam + p_if_ham)).abs() < 0.000001);
    }
}
```
